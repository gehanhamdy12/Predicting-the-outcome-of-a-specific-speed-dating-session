{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzXcb0fuPWXl"
      },
      "source": [
        "#Import libraries\n",
        "import all libraries which I used in my code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2t_zj2QPBOq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "from sklearn.datasets import make_classification\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRv2z_glPgjm"
      },
      "source": [
        "#Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBacb8q1BjDz"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "load train & test file and read these files using (pd.read_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "-O0Rox8HPSw2",
        "outputId": "6a1e54be-b479-4cb2-d782-0b1340ff3be1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-60dc214f-077b-427c-8209-6682c3940ad9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>...</th>\n",
              "      <th>attr3_3</th>\n",
              "      <th>sinc3_3</th>\n",
              "      <th>intel3_3</th>\n",
              "      <th>fun3_3</th>\n",
              "      <th>amb3_3</th>\n",
              "      <th>attr5_3</th>\n",
              "      <th>sinc5_3</th>\n",
              "      <th>intel5_3</th>\n",
              "      <th>fun5_3</th>\n",
              "      <th>amb5_3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2583</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>14</td>\n",
              "      <td>12</td>\n",
              "      <td>372.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6830</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>63.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4840</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>331.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5508</th>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>18</td>\n",
              "      <td>13.0</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>200.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4828</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>20</td>\n",
              "      <td>17</td>\n",
              "      <td>357.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3390</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>214.0</td>\n",
              "      <td>...</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4130</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>199.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>290.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5016</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "      <td>14.0</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>151.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8149</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>7</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>542.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5909 rows × 191 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60dc214f-077b-427c-8209-6682c3940ad9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60dc214f-077b-427c-8209-6682c3940ad9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60dc214f-077b-427c-8209-6682c3940ad9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
              "id                                                                           \n",
              "2583       0    3       2    14     18         2       2.0     14       12   \n",
              "6830       1   14       1     3     10         2       NaN      8        8   \n",
              "4840       1   14       1    13     10         8       8.0     10       10   \n",
              "5508       1   38       2     9     20        18      13.0      6        7   \n",
              "4828       1   24       2    14     20         6       6.0     20       17   \n",
              "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
              "3390       0    1       2     9     20         2       2.0     18        1   \n",
              "4130       1   24       2     9     20        19      15.0      5        6   \n",
              "1178       0   13       2    11     21         5       5.0      3       18   \n",
              "5016       1   10       2     7     16         6      14.0      9       10   \n",
              "8149       0    7       2    21     22         7       7.0      2       12   \n",
              "\n",
              "        pid  ...  attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  \\\n",
              "id           ...                                                        \n",
              "2583  372.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "6830   63.0  ...      6.0      8.0       8.0     7.0     8.0      NaN   \n",
              "4840  331.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "5508  200.0  ...      8.0      9.0       8.0     8.0     6.0      NaN   \n",
              "4828  357.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "...     ...  ...      ...      ...       ...     ...     ...      ...   \n",
              "3390  214.0  ...     12.0     12.0      12.0     9.0    12.0      NaN   \n",
              "4130  199.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "1178  290.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "5016  151.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "8149  542.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "\n",
              "      sinc5_3  intel5_3  fun5_3  amb5_3  \n",
              "id                                       \n",
              "2583      NaN       NaN     NaN     NaN  \n",
              "6830      NaN       NaN     NaN     NaN  \n",
              "4840      NaN       NaN     NaN     NaN  \n",
              "5508      NaN       NaN     NaN     NaN  \n",
              "4828      NaN       NaN     NaN     NaN  \n",
              "...       ...       ...     ...     ...  \n",
              "3390      NaN       NaN     NaN     NaN  \n",
              "4130      NaN       NaN     NaN     NaN  \n",
              "1178      NaN       NaN     NaN     NaN  \n",
              "5016      NaN       NaN     NaN     NaN  \n",
              "8149      NaN       NaN     NaN     NaN  \n",
              "\n",
              "[5909 rows x 191 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# read train data file \n",
        "# set 'id' as column index in train data\n",
        "train_data = pd.read_csv(\"train.csv\",index_col='id')\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "gbl0j5dvQt61",
        "outputId": "5b27feb6-36a6-4373-ad3f-f4e156d54669"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0a89f442-42b5-4049-8a77-1d1dbf391cec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>...</th>\n",
              "      <th>attr3_3</th>\n",
              "      <th>sinc3_3</th>\n",
              "      <th>intel3_3</th>\n",
              "      <th>fun3_3</th>\n",
              "      <th>amb3_3</th>\n",
              "      <th>attr5_3</th>\n",
              "      <th>sinc5_3</th>\n",
              "      <th>intel5_3</th>\n",
              "      <th>fun5_3</th>\n",
              "      <th>amb5_3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>934</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>52.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6539</th>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>368.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6757</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>212.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2275</th>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>30.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1052</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>162.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7982</th>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>18</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>407.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7299</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>339.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1818</th>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>23.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>215.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6691</th>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>7.0</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>513.0</td>\n",
              "      <td>...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows × 190 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a89f442-42b5-4049-8a77-1d1dbf391cec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a89f442-42b5-4049-8a77-1d1dbf391cec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a89f442-42b5-4049-8a77-1d1dbf391cec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
              "id                                                                           \n",
              "934        0    5       2     2     16         3       NaN     13       13   \n",
              "6539       0   33       2    14     18         6       6.0      4        8   \n",
              "6757       1    6       2     9     20        10      16.0     15       19   \n",
              "2275       1   26       2     2     19        15       NaN      8       10   \n",
              "1052       0   29       2     7     16         7       7.0     10        5   \n",
              "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
              "7982       0   23       2    15     19        18      18.0     14       11   \n",
              "7299       0    5       1    13      9         4       4.0      4        8   \n",
              "1818       1   26       2     2     19         3       NaN     15        3   \n",
              "937        0   19       2     9     20        11      11.0      9        2   \n",
              "6691       1   38       2    21     22        22       7.0     16        5   \n",
              "\n",
              "        pid  ...  attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  \\\n",
              "id           ...                                                        \n",
              "934    52.0  ...      5.0      7.0       8.0     6.0     8.0      NaN   \n",
              "6539  368.0  ...      6.0      8.0       7.0     7.0     8.0      6.0   \n",
              "6757  212.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "2275   30.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "1052  162.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "...     ...  ...      ...      ...       ...     ...     ...      ...   \n",
              "7982  407.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "7299  339.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "1818   23.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
              "937   215.0  ...      9.0      7.0      12.0    12.0     9.0      NaN   \n",
              "6691  513.0  ...      7.0      9.0       8.0     7.0     8.0      5.0   \n",
              "\n",
              "      sinc5_3  intel5_3  fun5_3  amb5_3  \n",
              "id                                       \n",
              "934       NaN       NaN     NaN     NaN  \n",
              "6539      7.0       6.0     5.0     5.0  \n",
              "6757      NaN       NaN     NaN     NaN  \n",
              "2275      NaN       NaN     NaN     NaN  \n",
              "1052      NaN       NaN     NaN     NaN  \n",
              "...       ...       ...     ...     ...  \n",
              "7982      NaN       NaN     NaN     NaN  \n",
              "7299      NaN       NaN     NaN     NaN  \n",
              "1818      NaN       NaN     NaN     NaN  \n",
              "937       NaN       NaN     NaN     NaN  \n",
              "6691      8.0       8.0     6.0     8.0  \n",
              "\n",
              "[2469 rows x 190 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# read test data file \n",
        "# set 'id' as column index in train data\n",
        "test_data = pd.read_csv(\"test.csv\",index_col='id')\n",
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e30OJROuCHeJ"
      },
      "source": [
        "## missing values in data \n",
        "\n",
        "first, I tried to find precentage of mising value in eatch column. then, I drop columns that have mising value more than 76%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdmDHCziSAH8"
      },
      "outputs": [],
      "source": [
        "# find precentage of mising value in each column in train data\n",
        "\n",
        "def precentage_missing_value(train_data): # define function to calculate the precentage\n",
        "  missing_value=pd.DataFrame(train_data.isnull().sum()/len(train_data))*100   # calculate the precentage of missing vall\n",
        "  missing_value.columns=['percentage of missing value in columns %']\n",
        "  missing_value['numbers of missing value in columns ']=pd.DataFrame(train_data.isnull().sum())  # sum missing value in data \n",
        "  return missing_value.sort_values(by='percentage of missing value in columns %',ascending=False) #get the precentage of missing vall \n",
        "    \n",
        "z=precentage_missing_value(train_data)\n",
        "#z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss8neBfWSAKM",
        "outputId": "c1cc872b-5540-4cf7-a105-e14e8eac2228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "columns that have more than 76% mising value = ['num_in_3', 'numdat_3', 'expnum', 'amb7_2', 'sinc7_2', 'shar7_2', 'attr7_2', 'intel7_2', 'fun7_2', 'shar7_3', 'attr7_3', 'sinc7_3', 'intel7_3', 'fun7_3', 'amb7_3', 'amb5_3', 'shar2_3', 'intel5_3', 'attr5_3', 'fun5_3', 'sinc5_3']\n"
          ]
        }
      ],
      "source": [
        "# drop columns that have mising values more than 76% percentage \n",
        "empty_col=[]\n",
        "for i in range (len(z)):\n",
        "  if(z['percentage of missing value in columns %'][i]) >76:\n",
        "    empty_col.append(z['percentage of missing value in columns %'].index[i])\n",
        "print(\"columns that have more than 76% mising value =\",empty_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzGEJjpJJ8Az"
      },
      "outputs": [],
      "source": [
        "# in transformation (always make a copy before you assign) train data\n",
        "train_data2 =train_data.copy() \n",
        "test_data2 =test_data.copy()\n",
        "\n",
        "# drop columns that have more than 76% missing values  in train data\n",
        "# drop this column (train_data['field]) because we have another column which is (train_data['field_cd']) that indicates the same meaning of it but in a numerical way \n",
        "# drop  ('idg','pid','zipcode')columns that  have no  impact // useless columns\n",
        "\n",
        "train_data2 = train_data.drop(['num_in_3','numdat_3','expnum','amb7_2','sinc7_2','shar7_2','fun7_2','intel7_2','attr7_2','attr7_3','sinc7_3','intel7_3','fun7_3','amb7_3','shar7_3',\n",
        " 'shar2_3','attr5_3','sinc5_3','intel5_3','fun5_3','amb5_3','career','field','idg','pid','zipcode'],axis=1)\n",
        "\n",
        "# drop columns that have more than 76% missing values  in test data\n",
        "test_data2 = test_data.drop(['num_in_3','numdat_3','expnum','amb7_2','sinc7_2','shar7_2','fun7_2','intel7_2','attr7_2','attr7_3','sinc7_3','intel7_3','fun7_3','amb7_3','shar7_3',\n",
        " 'shar2_3','attr5_3','sinc5_3','intel5_3','fun5_3','amb5_3','career','field','idg','pid','zipcode'],axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B35qKUAuHgMg"
      },
      "source": [
        "## convert object data to categorical\n",
        "frist, I tried to find columns which it's data type is object in train data &test data. then convert object data to catigorical (encoding to categorical) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "WLC05WhBQ1_z",
        "outputId": "15873f40-8174-46e3-b46c-8e1e05875815"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c48e1c31-9d4e-4e7f-b88e-9db4234e950f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>undergra</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>from</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2583</th>\n",
              "      <td>University of Michigan-Ann Arbor</td>\n",
              "      <td>1,290.00</td>\n",
              "      <td>21,645.00</td>\n",
              "      <td>Palo Alto, CA</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6830</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Boston, MA</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4840</th>\n",
              "      <td>Rizvi College of Architecture, Bombay University</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bombay, India</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5508</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>45,300.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4828</th>\n",
              "      <td>Harvard College</td>\n",
              "      <td>1,400.00</td>\n",
              "      <td>26,019.00</td>\n",
              "      <td>Midwest USA</td>\n",
              "      <td>46,138.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3390</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New York</td>\n",
              "      <td>65,708.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4130</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>University of Washington</td>\n",
              "      <td>1,155.00</td>\n",
              "      <td>13,258.00</td>\n",
              "      <td>Seattle</td>\n",
              "      <td>37,881.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5016</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Canada</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8149</th>\n",
              "      <td>Hamilton College</td>\n",
              "      <td>1,280.00</td>\n",
              "      <td>27,350.00</td>\n",
              "      <td>Cambridge, MA</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5909 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c48e1c31-9d4e-4e7f-b88e-9db4234e950f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c48e1c31-9d4e-4e7f-b88e-9db4234e950f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c48e1c31-9d4e-4e7f-b88e-9db4234e950f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              undergra    mn_sat    tuition  \\\n",
              "id                                                                            \n",
              "2583                  University of Michigan-Ann Arbor  1,290.00  21,645.00   \n",
              "6830                                               NaN       NaN        NaN   \n",
              "4840  Rizvi College of Architecture, Bombay University       NaN        NaN   \n",
              "5508                                               NaN       NaN        NaN   \n",
              "4828                                   Harvard College  1,400.00  26,019.00   \n",
              "...                                                ...       ...        ...   \n",
              "3390                                               NaN       NaN        NaN   \n",
              "4130                                               NaN       NaN        NaN   \n",
              "1178                          University of Washington  1,155.00  13,258.00   \n",
              "5016                                               NaN       NaN        NaN   \n",
              "8149                                  Hamilton College  1,280.00  27,350.00   \n",
              "\n",
              "                from     income  \n",
              "id                               \n",
              "2583   Palo Alto, CA        NaN  \n",
              "6830      Boston, MA        NaN  \n",
              "4840   Bombay, India        NaN  \n",
              "5508  Washington, DC  45,300.00  \n",
              "4828     Midwest USA  46,138.00  \n",
              "...              ...        ...  \n",
              "3390        New York  65,708.00  \n",
              "4130        Colombia        NaN  \n",
              "1178         Seattle  37,881.00  \n",
              "5016          Canada        NaN  \n",
              "8149   Cambridge, MA        NaN  \n",
              "\n",
              "[5909 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "train_data2.select_dtypes(include='object') # find columns which it's data type is object in train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "k6JnYrFmQ4ZU",
        "outputId": "0cc3b6e7-0763-42a1-bd97-d5985e29f071"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6977d9e0-5c82-4856-be16-069959b1f503\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>undergra</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>from</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>934</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hong Kong</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6539</th>\n",
              "      <td>wellesley college</td>\n",
              "      <td>1,341.00</td>\n",
              "      <td>25,504.00</td>\n",
              "      <td>atlanta, ga</td>\n",
              "      <td>36,223.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6757</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>55,080.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2275</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>26,482.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1052</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Atlanta, GA</td>\n",
              "      <td>21,590.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7982</th>\n",
              "      <td>Columbia</td>\n",
              "      <td>1,430.00</td>\n",
              "      <td>26,908.00</td>\n",
              "      <td>Hong Kong</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7299</th>\n",
              "      <td>Bucknell University</td>\n",
              "      <td>1,290.00</td>\n",
              "      <td>25,335.00</td>\n",
              "      <td>Erie, PA</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1818</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>26,482.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Vestal</td>\n",
              "      <td>42,640.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6691</th>\n",
              "      <td>LUISS, Rome</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Italy</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6977d9e0-5c82-4856-be16-069959b1f503')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6977d9e0-5c82-4856-be16-069959b1f503 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6977d9e0-5c82-4856-be16-069959b1f503');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 undergra    mn_sat    tuition           from     income\n",
              "id                                                                      \n",
              "934                   NaN       NaN        NaN      Hong Kong        NaN\n",
              "6539    wellesley college  1,341.00  25,504.00    atlanta, ga  36,223.00\n",
              "6757                  NaN       NaN        NaN  San Francisco  55,080.00\n",
              "2275                  NaN       NaN        NaN       Brooklyn  26,482.00\n",
              "1052                  NaN       NaN        NaN    Atlanta, GA  21,590.00\n",
              "...                   ...       ...        ...            ...        ...\n",
              "7982             Columbia  1,430.00  26,908.00      Hong Kong        NaN\n",
              "7299  Bucknell University  1,290.00  25,335.00       Erie, PA        NaN\n",
              "1818                  NaN       NaN        NaN       Brooklyn  26,482.00\n",
              "937                   NaN       NaN        NaN         Vestal  42,640.00\n",
              "6691          LUISS, Rome       NaN        NaN          Italy        NaN\n",
              "\n",
              "[2469 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "test_data2.select_dtypes(include='object')  # find columns which data type is object in test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF8TMNEAQ_aE"
      },
      "outputs": [],
      "source": [
        "# categorical encoding of gender\n",
        "\n",
        "# categorical encoding of undergra \n",
        "train_data2['undergra'] = train_data2['undergra'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of mn_sat \n",
        "train_data2['mn_sat'] = train_data2['mn_sat'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of tuition \n",
        "train_data2['tuition'] = train_data2['tuition'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of form \n",
        "train_data2['from'] = train_data2['from'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of zipcode \n",
        "train_data2['zipcode'] = train_data2['zipcode'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of income\t \n",
        "train_data2['income'] = train_data2['income'].astype(\"category\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWqVGF5MS2x8"
      },
      "outputs": [],
      "source": [
        "# categorical encoding of gender\n",
        "\n",
        "# categorical encoding of undergra \n",
        "test_data2['undergra'] = test_data2['undergra'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of mn_sat \n",
        "test_data2['mn_sat'] = test_data2['mn_sat'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of tuition \n",
        "test_data2['tuition'] = test_data2['tuition'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of form \n",
        "test_data2['from'] = test_data2['from'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of zipcode \n",
        "test_data2['zipcode'] = test_data2['zipcode'].astype(\"category\")\n",
        "\n",
        "# categorical encoding of income\t \n",
        "test_data2['income'] = test_data2['income'].astype(\"category\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfpefqeS20L",
        "outputId": "b506dd85-44a3-4bb3-d438-20d25c2ff2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original shape (5909, 164) (5909,)\n"
          ]
        }
      ],
      "source": [
        "y= train_data2['match'] # (match) column for vector\n",
        "X =train_data2.drop('match', axis=1) # all columns except (match) column for matrix\n",
        "print('original shape', X.shape, y.shape) # show shap of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQaIqhYgVF6P",
        "outputId": "bb35a3f8-37cd-428f-b349-e8a62e66412d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5909 entries, 2583 to 8149\n",
            "Columns: 165 entries, gender to amb3_3\n",
            "dtypes: float64(151), int64(9), object(5)\n",
            "memory usage: 7.5+ MB\n"
          ]
        }
      ],
      "source": [
        "train_data2.info() # know data type of each column in data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mG-YZfLwGN"
      },
      "source": [
        "#Pipeline\n",
        "\n",
        "A machine learning pipeline is used to help automate machine learning workflows, They work by allowing a set of data to be transformed and associated in a model\n",
        "\n",
        "Any preprocessing steps, in fact, might be regarded part of the model, with many configurations for each step.\n",
        "\n",
        "We combine preprocessing steps and models into a single tunable pipeline using hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUMmyAzDUfly",
        "outputId": "ac7020cd-6dad-4560-8608-e2d614965197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numeric features: ['gender', 'condtn', 'wave', 'round', 'position', 'positin1', 'order', 'partner', 'int_corr', 'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o', 'age', 'field_cd', 'race', 'imprace', 'imprelig', 'goal', 'date', 'go_out', 'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1', 'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob', 'met', 'match_es', 'attr1_s', 'sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s', 'sinc3_s', 'intel3_s', 'fun3_s', 'amb3_s', 'satis_2', 'length', 'numdat_2', 'attr1_2', 'sinc1_2', 'intel1_2', 'fun1_2', 'amb1_2', 'shar1_2', 'attr4_2', 'sinc4_2', 'intel4_2', 'fun4_2', 'amb4_2', 'shar4_2', 'attr2_2', 'sinc2_2', 'intel2_2', 'fun2_2', 'amb2_2', 'shar2_2', 'attr3_2', 'sinc3_2', 'intel3_2', 'fun3_2', 'amb3_2', 'attr5_2', 'sinc5_2', 'intel5_2', 'fun5_2', 'amb5_2', 'you_call', 'them_cal', 'date_3', 'attr1_3', 'sinc1_3', 'intel1_3', 'fun1_3', 'amb1_3', 'shar1_3', 'attr4_3', 'sinc4_3', 'intel4_3', 'fun4_3', 'amb4_3', 'shar4_3', 'attr2_3', 'sinc2_3', 'intel2_3', 'fun2_3', 'amb2_3', 'attr3_3', 'sinc3_3', 'intel3_3', 'fun3_3', 'amb3_3']\n",
            "categorical features: []\n"
          ]
        }
      ],
      "source": [
        "# here I extract numeric features and categorical features names\n",
        "\n",
        "# numeric features can be selected by: (based on the train_data2.info() output )\n",
        "features_numeric = list(X.select_dtypes(include=['float64', 'int64']))\n",
        "\n",
        "# categorical features can be selected by: (based on the train_data2.info() output )\n",
        "features_categorical = list(X.select_dtypes(include=['category']))\n",
        "\n",
        "print('numeric features:', features_numeric)\n",
        "print('categorical features:', features_categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What preprocessing steps are used?\n",
        "\n",
        "**Handling the missing data**\n",
        "\n",
        "here we handled any missing value using **SimpleImputer** which is a scikit-learn class which is helpful in handling the missing data in the predictive model dataset.\n",
        "\n",
        "**Applying StandardScaler** to resize the distribution of values in the data set \n",
        "\n",
        "**Applying OneHotEncoder** to convert categorical features as a one-hot numeric array.\n",
        "\n"
      ],
      "metadata": {
        "id": "iut1yrA1ydkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i2MaFnpVV6x"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# define a pipe line for numeric feature preprocessing\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer()),\n",
        "        ('scaler', StandardScaler())]\n",
        ")\n",
        "\n",
        "# define a pipe line for categorical feature preprocessing\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]\n",
        ")\n",
        "# define the preprocessor \n",
        "# we gave them a name so we can set their hyperparameters\n",
        "# we also specify what are the categorical \n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric),\n",
        "        ('cat', transformer_categorical, features_categorical)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the experimental protocol used and how was it carried out?**\n",
        "\n",
        "here I used Cross validation with (random ,bayes , grid search ) and using validation set \n",
        "\n",
        "Cross–validation is a method of evaluating a machine learning model's ability to predict new data.\n",
        "\n",
        "Validation set is a set of data that is separate from the training set and is used to verify the performance of our model during training."
      ],
      "metadata": {
        "id": "jAIvAqHy1TMU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7n4McJcPao7"
      },
      "source": [
        "#XGBoost Classifier Model \n",
        "XGBoost machine learning models provide the best combination of prediction performance and processing time when compared to other algorithms, It's also a library for creating gradient boosting tree models that are both quick and high-performing. That XGBoost outperforms the competition on a variety of tough machine learning tasks becuse it is  powerful enough to deal with all sorts of irregularities of data. So I will use this model to try to get a highly prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4uMRfwhifj-"
      },
      "outputs": [],
      "source": [
        "# combine the preprocessor with the model as a full tunable pipeline\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier', \n",
        "           XGBClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "full_pipline\n",
        "full_pipline=full_pipline.fit(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N8u1E9vYG32"
      },
      "source": [
        "**To improve and get fully use the benefits  of  XGBoost model,  hyper  parameter tuning is must.**\n",
        "\n",
        "And It is very difficult to get answers to practical questions like\n",
        "\n",
        "*   Which set of parameters you should tune ?\n",
        "*   What is the ideal value of these parameters to obtain optimal output ? \n",
        "\n",
        "So I tried to tune a list of parameters of XGBoost hyper parameter such as   **(learning_rate, max_depth , gamma ,n_estimators)**\n",
        "\n",
        "**learning_rate :**Step size shrinkage used in update to prevents overfitting.[1]\n",
        "\n",
        "it's value must be between 0 and 1 to optimizes the chances to reach the best optimum\n",
        "\n",
        "I choosed a list that has values = [0.01,0.05,0.1,0.4] \n",
        "\n",
        "that it lay on the range from 0.01 to 0.3 \n",
        "\n",
        "and it's Default = 0.3. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**max_depth:** which mean the maximum depth of a tree we use it.\n",
        "\n",
        "if I use a high max_depth, performance  might increase but the model's complexity will increase and lead to overfit on the other hand if I choose a very small max_depth if made my model can't able to learn well and lead to underfit \n",
        "so I choosed a list of max_depth paramters to get the most suitable max_depth to make our model increase it's performance \n",
        "\n",
        "I choosed a list that has values = [5,6,7,9]\n",
        "\n",
        "**gamma:** which mean  is a pseudo-regularisation parameter (Lagrangian multiplier), and depends on the other parameters. The higher Gamma is, the higher the regularization. It can be any integer. Default is 0[3]\n",
        "\n",
        "I choosed a list that has values = [ 0.1, 0.2 , 0.3]\n",
        "\n",
        "\n",
        "**n_estimators:** which mean  number of trees in our ensemble.\n",
        "\n",
        "why we use this hyper parameter in our model?!!!\n",
        "\n",
        "The reason is in the way that the boosted tree model is constructed, sequentially where each new tree attempts to model and correct for the errors made by the sequence of previous trees.[2]\n",
        "\n",
        "I choosed a list that has values = [50,100,150,200,250] \n",
        "\n",
        "it's Default = 100.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft_SigUNXiSA"
      },
      "outputs": [],
      "source": [
        "# set of hyperparameters wich use with XBBoost classifier model to get high performance\n",
        "# # `__` denotes an attribute of the preceeding name\n",
        "param_grid = {\n",
        " 'preprocessor__num__imputer__strategy': ['mean'],  # preprocessor__num__imputer__strategy points to preprocessor->num (a Pipeline)-> imputer -> strategy\n",
        "  'my_classifier__learning_rate' : [0.01,0.05,0.1,0.3], # my_classifier__n_estimators points to my_classifier->learning_rate\n",
        " 'my_classifier__max_depth' : [5,6,7,9],                # my_classifier__n_estimators points to my_classifier->max_depth \n",
        " 'my_classifier__gamma': [ 0.1, 0.2 , 0.3],             # my_classifier__n_estimators points to my_classifier->gamma\n",
        " 'my_classifier__n_estimators': [50,100,150,200,250]    # my_classifier__n_estimators points to my_classifier->n_estimators \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0-xnNbSYuUY"
      },
      "source": [
        "##Trial_1 using XGBoost model with Random Search method\n",
        "\n",
        "**Random search** is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. and yet it has proven to yield better results comparatively.[4] So I trid to use rondom search to get better results and high accuracy.\n",
        "\n",
        "**Expectation**\n",
        "\n",
        "I tried random search with XGBoost classifier model and set of hyperparameters which must be tune to get a high performance with XGBoost model.\n",
        "\n",
        "my expectation that I will get a high accuraccy \n",
        "\n",
        "\n",
        "**Observation** \n",
        "\n",
        "After running the model I found that the best hyperparameters that defined with rondom search method are\n",
        "\n",
        "n_estimators = 250\n",
        "\n",
        "max_depth = 6\n",
        "\n",
        "learning_rate = 0.05\n",
        "\n",
        "gamma =  0.2\n",
        "\n",
        "'best score 0.8832890429918686 ---> that the model given me \n",
        "\n",
        "score on kaggle = Score: 0.88061 ---> I got on kaggle\n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will try to use bayesian search method with the XGBoost and the same set of hyperparameter \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIDDk7ZBXqXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5d11b3-2f57-4c26-b32a-d4be21d544bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
            "best score 0.8832890429918686\n",
            "best score {'preprocessor__num__imputer__strategy': 'mean', 'my_classifier__n_estimators': 250, 'my_classifier__max_depth': 6, 'my_classifier__learning_rate': 0.05, 'my_classifier__gamma': 0.2}\n"
          ]
        }
      ],
      "source": [
        "# define rondom search\n",
        "rondom_search = RandomizedSearchCV(\n",
        "    full_pipline, param_grid, cv=10  #cv=10 means two-fold cross-validation\n",
        "     , verbose=1, n_jobs=2,          # n_jobs means the cucurrent number of jobs\n",
        "    # number of random trials\n",
        "    n_iter=10,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "rondom_search.fit(X, y)\n",
        "\n",
        "print('best score {}'.format(rondom_search.best_score_))\n",
        "print('best score {}'.format(rondom_search.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHA5rs04Xqxw"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['match'] = rondom_search.predict_proba(test_data)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_walkthrough_1.csv', index=False) #  Score: 0.88061 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX_lCSEhuS0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6e39d5-8fb7-4aca-c1c7-a3c877e32612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 100 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.2)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jABUoxCbikaL"
      },
      "source": [
        "##Trial_2 using XGBoost model with bayesian search method with Cross validation \n",
        "Bayesian optimization methods are efficient because they select hyperparameters in an informed manner. By prioritizing hyperparameters that appear more promising from past results, Bayesian methods can find the best hyperparameters in lesser time (in fewer iterations)[5] So I tried to use Bayes search method to get high performance .\n",
        "\n",
        "**Expectation**\n",
        "\n",
        "I expect that my model may get a high accuracy and make a very good prediction with using XGBoost model with appropiate \n",
        "hyperparametr and bayes search method\n",
        "\n",
        "**Observation**\n",
        "\n",
        "after running the model I found that the best hyperparameter that defined with XGBoost classifier using bayes search method is \n",
        "\n",
        "gamma = 0.2\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "max_depth = 7\n",
        "\n",
        "n_estimators = 50\n",
        "\n",
        "\n",
        "best score 0.8674900341277352\n",
        "\n",
        "Score: 0.87616 on kaggle\n",
        "\n",
        "The accuracy  was lower than the accuracy I got with using random search and the hyperparameter are changed \n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will use the same  method (bayes search method) use validation set insted of cross validation with the same Classifer (XGBoost model) and the same set of hyperparameter. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8kLjBrkt65G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a3c630-86f6-4a9b-a628-c5d9d8f98c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "best score 0.8674900341277352\n",
            "best score OrderedDict([('my_classifier__gamma', 0.2), ('my_classifier__learning_rate', 0.1), ('my_classifier__max_depth', 7), ('my_classifier__n_estimators', 50), ('preprocessor__num__imputer__strategy', 'mean')])\n"
          ]
        }
      ],
      "source": [
        "from skopt import BayesSearchCV\n",
        "\n",
        "# define ranges for bayes search\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    param_grid,\n",
        "    # number of trials \n",
        "    n_iter=10,\n",
        "    random_state=0,\n",
        "    verbose=1,\n",
        "    #cv=10 means ten-fold cross-validation\n",
        "    cv=10,\n",
        ")\n",
        "\n",
        "bayes_search.fit(X, y)\n",
        "\n",
        "print('best score {}'.format(bayes_search.best_score_))\n",
        "print('best score {}'.format(bayes_search.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr-znGXDugbs"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['match'] = bayes_search.predict_proba(test_data)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_walkthrough_2.csv', index=False) # Score: 0.87616 on kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial_3 using XGBoost model with Bayes Search method use  validation set\n",
        "\n",
        "**Expectation**\n",
        "\n",
        "I expect that using bayes search method with validation set using XGBoost model and the same set hyperparameter may be faster than using bayes search with cross validation (take smaller time in running the code) and may give me higher accuracy\n",
        "\n",
        "**Observation**\n",
        "\n",
        "after running the model I found that The accuracy was the higher than using bayes search with cross validation and the model run faster than using bayes search with cross validation \n",
        "\n",
        "best hyperparameter gamma = 0.3\n",
        "\n",
        "learning_rate = 0.3\n",
        "\n",
        "max_depth = 5 \n",
        "\n",
        "n_estimators = 150\n",
        "\n",
        "preprocessor__num__imputer__strategy ='mean')\n",
        "\n",
        "\n",
        "best score 0.8870976020366265 --->  that model given me\n",
        "\n",
        "Score on kaggle: 0.87795  ----> I got on kaggle\n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will use another methode(grid search) with the same model XGBoost model with the same hyperparameter"
      ],
      "metadata": {
        "id": "Ralj1PNLtc54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Further split the original training set to a train and a validation set\n",
        "X_train2, X_val, y_train2, y_val = train_test_split(\n",
        "    X, y, train_size = 0.8, stratify = y, random_state = 2022)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train2.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "\n",
        "barys_search_= BayesSearchCV(\n",
        "    full_pipline, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
        "    scoring='roc_auc')\n",
        "\n",
        "# here we still use X_train; but the grid search model\n",
        "# will use our predefined split internally to determine \n",
        "# which sample belongs to the validation set\n",
        "barys_search_.fit(X, y)\n",
        "\n",
        "print('best score {}'.format(barys_search_.best_score_))\n",
        "print('best score {}'.format(barys_search_.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAKAUbcFkpZi",
        "outputId": "9fc6b974-3bea-4720-edad-9ce22cb93c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "best score 0.8870976020366265\n",
            "best score OrderedDict([('my_classifier__gamma', 0.3), ('my_classifier__learning_rate', 0.3), ('my_classifier__max_depth', 5), ('my_classifier__n_estimators', 150), ('preprocessor__num__imputer__strategy', 'mean')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['match'] = barys_search_.predict_proba(test_data)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_walkthrough_6.csv', index=False) #Score: 0.87795 on kaggle\n",
        "\n"
      ],
      "metadata": {
        "id": "e4k32q1GkphA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj19IK7Evuy7"
      },
      "source": [
        "##Trail_4  using XGBoost model with Grid Search method use cross validation\n",
        "\n",
        "We can tune the hyperparameters (including different preprocessing configurations using **cross-validation and grid-search**).\n",
        "\n",
        "**Grid search** is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on a the specific parameter values of a model.[6] \n",
        "\n",
        "here I used grid search method with XGBoost classifier model to tune the hyperparameter to get high accurracy \n",
        "\n",
        "**Expectation**\n",
        "\n",
        "I expect that using gride search method with cross validation with XGBoost model and the same set hyperparamete may give me a higher accuracy \n",
        "\n",
        "**Observation**\n",
        "\n",
        "after running the model I found that The accuracy was the highest one I got when using XGBoost classifier model and the most appropiat hyperpater with XGBoost model is \n",
        "\n",
        "gamma: 0.1\n",
        "\n",
        "learning_rate: 0.05\n",
        "\n",
        "max_depth: 5\n",
        "\n",
        "n_estimators: 250 \n",
        "\n",
        "best score 0.8850648975774842 ---> that model given me\n",
        "\n",
        "on kaggle Score: 0.88625 ----> I got on kaggle\n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will use another model (Logistic regrission) with set of hyperparameter and with different methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUCeNzbKvrJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddf4c0a-8247-4845-c629-8479043d1020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
            "best score 0.8850648975774842\n",
            "best score {'my_classifier__gamma': 0.1, 'my_classifier__learning_rate': 0.05, 'my_classifier__max_depth': 5, 'my_classifier__n_estimators': 250, 'preprocessor__num__imputer__strategy': 'mean'}\n"
          ]
        }
      ],
      "source": [
        "# Grid Search with Cross-validation\n",
        "grid_search_cross = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=5, verbose=1, n_jobs=2, # cv=5 means two-fold cross-validation\n",
        "                                                        # n_jobs means the cucurrent number of jobs\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search_cross.fit(X, y)\n",
        "\n",
        "print('best score {}'.format(grid_search_cross.best_score_))\n",
        "print('best score {}'.format(grid_search_cross.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SMuXIJkvrPd"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data2.index\n",
        "\n",
        "submission['match'] = grid_search_cross.predict_proba(test_data2)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_walkthrough_3.csv', index=False)  #Score: 0.87440  on kaggle\n",
        "\n",
        "# best score 0.8844237237290882\n",
        "# best score {'my_classifier__gamma': 0.1, 'my_classifier__learning_rate': 0.1, 'my_classifier__max_depth': 9, 'my_classifier__n_estimators': 250, 'preprocessor__num__imputer__strategy': 'mean'}\n",
        "# on kaggle Score: 0.88625"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyPlGM0_-2mg"
      },
      "source": [
        "# LogisticRegression Classifier Model\n",
        "\n",
        "We use Logistic regression to understand the relationship between the dependent variable and one or more independent variables by estimating probabilities using a logistic regression equation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIjTYrKb1wKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d224a2c9-1f4e-4bbb-e8b0-ca9f388cbc43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('num',\n",
              "                                                  Pipeline(steps=[('imputer',\n",
              "                                                                   SimpleImputer()),\n",
              "                                                                  ('scaler',\n",
              "                                                                   StandardScaler())]),\n",
              "                                                  ['gender', 'idg', 'condtn',\n",
              "                                                   'wave', 'round', 'position',\n",
              "                                                   'positin1', 'order',\n",
              "                                                   'partner', 'pid', 'int_corr',\n",
              "                                                   'samerace', 'age_o',\n",
              "                                                   'race_o', 'pf_o_att',\n",
              "                                                   'pf_o_sin', 'pf_o_int',\n",
              "                                                   'pf_o_fun', 'pf_o_amb',\n",
              "                                                   'pf_o_sha', 'attr_o',\n",
              "                                                   'sinc_o', 'intel_o', 'fun_o',\n",
              "                                                   'amb_o', 'shar_o', 'like_o',\n",
              "                                                   'prob_o', 'met_o', 'age', ...]),\n",
              "                                                 ('cat',\n",
              "                                                  Pipeline(steps=[('imputer',\n",
              "                                                                   SimpleImputer(strategy='constant')),\n",
              "                                                                  ('onehot',\n",
              "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
              "                                                  ['field', 'undergra',\n",
              "                                                   'mn_sat', 'tuition', 'from',\n",
              "                                                   'zipcode', 'income'])])),\n",
              "                ('my_classifier', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# combine the preprocessor with the model as a full tunable pipeline\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier', \n",
        "           LogisticRegression(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "full_pipline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning** \n",
        "\n",
        " Hyperparameters are an essential aspect of machine learning process. as they control the overall behaviour of a machine learning model. The ultimate goal is to find an optimal combination of hyperparameters  to give better results.\n",
        "\n",
        "So I tried to tune a list of parameters of Logisticregression's  hyper parameter such as **( penalty , C , solver )**\n",
        "\n",
        "**penalty** it's values is  ‘l1’, ‘l2’, ‘elasticnet’, ‘none’.\n",
        "\n",
        "Penalty is used to specify the norm used in the penalization \n",
        "\n",
        "The default value is ’l2’.\n",
        "\n",
        "**C** which mean inverse of regularization strength in Logistic Regression.and \n",
        " it be in range ( 0.001 , 0.01 , 0.1 , 1 , 10 )\n",
        "\n",
        "**solver** It provides options to choose solver algorithm for optimization.\n",
        "it's parameter 'liblinear','newton-cg', 'lbfgs'\n"
      ],
      "metadata": {
        "id": "eZxi4BejOoDg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l3q6i-w22O5"
      },
      "outputs": [],
      "source": [
        "param_grid={\n",
        "    'preprocessor__num__imputer__strategy': ['mean'],\n",
        "    'my_classifier__penalty' : ['none','l1','l2','elasticnet'],\n",
        "    'my_classifier__C' : [100, 10, 1.0, 0.1, 0.01],\n",
        "    'my_classifier__solver': [ 'liblinear','newton-cg', 'lbfgs']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADYzGA-v1seo"
      },
      "source": [
        "##Trial_5 using Logistic Regression model with random search method\n",
        "\n",
        "**Random search** is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. and yet it has proven to yield better results comparatively.[4]\n",
        " So I trid to use rondom search to get better results and high accuracy. \n",
        "\n",
        "**Expectation**\n",
        "\n",
        "I tried random search with Logistic regression classifer model and list of hyperparameters tuned to get a high performance with Logistic regression model.\n",
        "\n",
        "my expectation that the accuraccy may not be very high like using XGBoost model\n",
        "\n",
        "\n",
        "**Observation** \n",
        "\n",
        "After running the model I found that the best hyperparameters that defined with random search method are\n",
        "\n",
        "C =  0.01\n",
        "\n",
        "penalty = l2\n",
        "\n",
        "solver' = newton-cg\n",
        "\n",
        "preprocessor__num__imputer__strategy'=  'mean'\n",
        "\n",
        "\n",
        "best score 0.8594276094276095 ---> that the model given me \n",
        "\n",
        "Score: 0.86986 on kaggle ---> I got on kaggle\n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will try to use grid search with (validation set) method using  Logistic regression model and the same set of hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jayTTupu22iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42baa35a-9817-4e94-aa75-b0aec0659c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
            "best score 0.8594276094276095\n",
            "best score {'preprocessor__num__imputer__strategy': 'mean', 'my_classifier__solver': 'newton-cg', 'my_classifier__penalty': 'l2', 'my_classifier__C': 0.01}\n"
          ]
        }
      ],
      "source": [
        "#Further split the original training set to a train and a validation set\n",
        "X_train2, X_val, y_train2, y_val = train_test_split(\n",
        "    X, y, train_size = 0.8, stratify = y, random_state = 2022)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train2.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "#\n",
        "random_search_lr = RandomizedSearchCV(\n",
        "    full_pipline, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    n_iter=10,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "random_search_lr.fit(X, y)\n",
        "\n",
        "print('best score {}'.format(random_search_lr.best_score_))\n",
        "print('best score {}'.format(random_search_lr.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv-a6MF53qxP"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data2.index\n",
        "\n",
        "submission['match'] = random_search_lr.predict_proba(test_data2)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_walkthrough_4.csv', index=False) # Score: 0.86986 on kaggle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trail_6:  using Logistic Regression model with grid search method\n",
        "\n",
        "In **grid search**, we test every combination of values from a pre-defined list of hyper-parameters and pick the best one based on the cross validation score or validation set \n",
        "\n",
        "**Expectation**\n",
        "\n",
        "I tried grid search with Logistic regression  classifier model and set of hyperparameters tuned to get a high performance with Logistic regression model.\n",
        "\n",
        "to find which method with Logistic regression is better (random search or grid search)\n",
        "\n",
        "my expectation that the accuraccy may be higher than using Logistic regression  model with grid search method  \n",
        "\n",
        "\n",
        "**Observation** \n",
        "\n",
        "A fter running the model I found that the best hyperparameters that defined with grid search method are\n",
        "\n",
        "using grid search method with Logistic regression classifier gives me higher accuracy than using random search method\n",
        "\n",
        "best hyperparameters\n",
        "C = 0.01\n",
        "\n",
        "penalty = l1\n",
        "\n",
        "solver = liblinear\n",
        "\n",
        "preprocessor__num__imputer__strategy= 'mean'\n",
        "\n",
        "best score 0.8729965060174144 ---> the model given me \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q5jk5xyf9buQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO4N-RLRz3H0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4456b6-b825-4f25-93c5-d3f3d5560fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 30 candidates, totalling 30 fits\n",
            "best score 0.8729965060174144\n",
            "best score {'my_classifier__C': 0.01, 'my_classifier__penalty': 'l1', 'my_classifier__solver': 'liblinear', 'preprocessor__num__imputer__strategy': 'mean'}\n"
          ]
        }
      ],
      "source": [
        "# Further split the original training set to a train and a validation set\n",
        "X_train2, X_val, y_train2, y_val = train_test_split(\n",
        "    X, y, train_size = 0.8, stratify = y, random_state = 2022)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train2.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "\n",
        "grid_search_lr = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
        "    scoring='roc_auc')\n",
        "\n",
        "# here we still use X_train; but the grid search model\n",
        "# will use our predefined split internally to determine \n",
        "# which sample belongs to the validation set\n",
        "grid_search_lr.fit(X, y)\n",
        "\n",
        "print('best score {}'.format(grid_search_lr.best_score_))\n",
        "print('best score {}'.format(grid_search_lr.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtNMaJA50ZfS"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['match'] = grid_search_lr.predict_proba(test_data)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_walkthrough_5.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b97gAAJn0U99"
      },
      "source": [
        "# Problem Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM1Dn0iX0xwd"
      },
      "source": [
        "**Define the problem:** is predicting the outcome of a specific speed dating session based on the profile of two people and implement a recommendation system to better match people in speed dating events but the given data has a lot of missing values that needed to handel .\n",
        "\n",
        "**What is the input?**\n",
        "our inputs is a set of features (information about the dating session) which help us to predict the match\n",
        "\n",
        "**What is the output?** The output is a probability predection (0-1) that the dating session will lead to a successful match or not \n",
        "\n",
        "**What data mining function is required?** The data mining function is binary classification \n",
        "\n",
        "**What could be the challenges?**  \n",
        "The challenges are that : \n",
        "\n",
        "data  has a lot of missing values and we should handel it \n",
        "\n",
        "dataset is highly unbalanced (mostly unmatched)\n",
        "\n",
        "need to implement a model to make a prediction of match or not and search for the hyperparameters to get the higher performance of our model \n",
        "\n",
        "**What is the impact?** The impact is that implement a model (make application) that can predict if the match will happend or not based on questions the both side answer \n",
        "\n",
        "**What is an ideal solution?**\n",
        "\n",
        "I found The which given me  ideal dolution (the highest accuracy ) was XGBoost model with Grid Search method use cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Questions"
      ],
      "metadata": {
        "id": "qmSUBBGZD_iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression?**\n",
        "\n",
        "Linear Regression is unsuitable for classification for two reasons. The first is that Linear Regression is dealing with continuous values, whereas classification problems demand discrete values. The second problem is that when new data points are added, the threshold value shifts.\n",
        "\n",
        "Unlike Linear Regression, Logistic Regression works with discrete values and keeps the threshold value constant even when new data points are introduced. For classification tasks, logistic regression is preferable.\n",
        "\n",
        "perceptron prophesy a binary class label with sign(wTxi), whereas linear regression predicts a real value with (wTxi).\n",
        "\n",
        "\n",
        "**2- What's a decision tree and how it is different to a logistic regression model?**\n",
        "\n",
        "**A decision tree** is a sort of supervised machine learning that categorises or predicts outcomes based on the answers to a previous set of questions.\n",
        "\n",
        "**how it is different to a logistic regression model ?**\n",
        "\n",
        "*   Decision tree can be trained on a small training set but Logistic regression requires a large enough training set\n",
        "*   Decision Trees are a non-parametric model with no pre-defined parameters. and does feature selection implicitly opposite to  Logistic regression which is a parametric model defined by having parameters multiplied by independent variables to predict the dependent variable.\n",
        "\n",
        "\n",
        "\n",
        "*   In Decission tree there is no  assumptions on the underlying distribution of the data...\n",
        "other than assumptions are made on response (or dependent) variable, with binomial or Bernoulli distribution in Logistic regression\n",
        "\n",
        "\n",
        "**3-What's the difference between grid search and random search?**\n",
        "\n",
        "**Grid search** In grid search, we test every combination of values from a \n",
        "pre-defined list of hyper-parameters and pick the best one based on the cross validation score or validation set \n",
        "\n",
        "**Random search** It tries a variety of different values at random (we have to define the number iterations).\n",
        "It is effective at evaluating a large range of values and, in most cases, quickly arrives at a very good combination; however, it does not guarantee the optimal parameter combination\n",
        "\n",
        "\n",
        "**4-What's the difference between bayesian search and random search?**\n",
        "\n",
        "**Bayesian Search** is a method of solving a search problem that is more efficient than randomly searching all options.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vyp5wJ5TEzZO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EWeYBg7ltP_"
      },
      "source": [
        "#References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA4LxZrY-SVI"
      },
      "source": [
        "[1] https://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "\n",
        "[2] https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
        "\n",
        "[3] https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n",
        "\n",
        "[4] https://analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/#:~:text=Random%20search%20is%20a%20technique,yields%20high%20variance%20during%20computing.\n",
        "\n",
        "[5] https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1\n",
        "\n",
        "[6] https://medium.com/fintechexplained/what-is-grid-search-c01fe886ef0a"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "0EWeYBg7ltP_"
      ],
      "name": "competition_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}